{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc344d24",
   "metadata": {},
   "source": [
    "# YouTube Spam stack model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cfba00f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9836182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z= zipfile.ZipFile(\"youtube+spam+collection (1).zip\")\n",
    "Psy = pd.read_csv(z.open(\"Youtube01-Psy.csv\"))\n",
    "Katy = pd.read_csv(z.open(\"Youtube02-KatyPerry.csv\"))\n",
    "LMFAO = pd.read_csv(z.open(\"Youtube03-LMFAO.csv\"))\n",
    "Eminem = pd.read_csv(z.open(\"Youtube04-Eminem.csv\"))\n",
    "Shakira = pd.read_csv(z.open(\"Youtube05-Shakira.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92d6be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([Psy, Katy, LMFAO, Eminem, Shakira])\n",
    "data.drop([\"COMMENT_ID\", \"AUTHOR\", \"DATE\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6fe2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"CONTENT\"], data[\"CLASS\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7be89140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1467, 3746)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(use_idf=True, lowercase= True)\n",
    "X_train_tfidf = tfidf_vect.fit_transform(X_train)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a7d398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tfidf_vocabulary(tfidf_vectorizer, filename):\n",
    "    with open(filename, 'wb') as vocab_file:\n",
    "        pickle.dump(tfidf_vectorizer.vocabulary_, vocab_file)\n",
    "\n",
    "# Save the TF-IDF vocabulary to a file\n",
    "save_tfidf_vocabulary(tfidf_vect, 'tfidf_vocabulary.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5750f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorizer saved to tfidf_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the TF-IDF vectorizer to a pickle file\n",
    "joblib.dump(tfidf_vect, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "# Verify that the vectorizer has been saved\n",
    "print(\"TF-IDF vectorizer saved to tfidf_vectorizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa7a969",
   "metadata": {},
   "source": [
    "# Classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09447ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c449f275",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1db8bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Accuracy: 0.9120654396728016\n",
      "- MCC: 0.837202504494492\n",
      "- F1 score: 0.9115440939602026\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.8936605316973415\n",
      "- MCC: 0.7992257112749366\n",
      "- F1 score: 0.8934291190139685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create and train the Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "dt.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = dt.predict(X_train_tfidf)\n",
    "y_test_pred = dt.predict(tfidf_vect.transform(X_test))\n",
    "\n",
    "# Training set performance\n",
    "dt_train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "dt_train_mcc = matthews_corrcoef(y_train, y_train_pred)\n",
    "dt_train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "# Test set performance\n",
    "dt_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "dt_test_mcc = matthews_corrcoef(y_test, y_test_pred)\n",
    "dt_test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Accuracy: %s' % dt_train_accuracy)\n",
    "print('- MCC: %s' % dt_train_mcc)\n",
    "print('- F1 score: %s' % dt_train_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Accuracy: %s' % dt_test_accuracy)\n",
    "print('- MCC: %s' % dt_test_mcc)\n",
    "print('- F1 score: %s' % dt_test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5318e942",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b9aa028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Accuracy: 0.9938650306748467\n",
      "- MCC: 0.9878030529094353\n",
      "- F1 score: 0.9938654925584773\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9406952965235174\n",
      "- MCC: 0.8848498924374916\n",
      "- F1 score: 0.9407265506491967\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create and train the Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=10)\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = rf.predict(X_train_tfidf)\n",
    "y_test_pred = rf.predict(tfidf_vect.transform(X_test))\n",
    "\n",
    "# Training set performance\n",
    "rf_train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "rf_train_mcc = matthews_corrcoef(y_train, y_train_pred)\n",
    "rf_train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "# Test set performance\n",
    "rf_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "rf_test_mcc = matthews_corrcoef(y_test, y_test_pred)\n",
    "rf_test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Accuracy: %s' % rf_train_accuracy)\n",
    "print('- MCC: %s' % rf_train_mcc)\n",
    "print('- F1 score: %s' % rf_train_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Accuracy: %s' % rf_test_accuracy)\n",
    "print('- MCC: %s' % rf_test_mcc)\n",
    "print('- F1 score: %s' % rf_test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a94ead5",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "473db84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Accuracy: 0.9918200408997955\n",
      "- MCC: 0.9836699249289926\n",
      "- F1 score: 0.9918205199162735\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.934560327198364\n",
      "- MCC: 0.8694245748054071\n",
      "- F1 score: 0.9346019875697238\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Create and train the MLP classifier\n",
    "mlp = MLPClassifier(alpha=1, max_iter=1000)\n",
    "mlp.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = mlp.predict(X_train_tfidf)\n",
    "y_test_pred = mlp.predict(tfidf_vect.transform(X_test))\n",
    "\n",
    "# Training set performance\n",
    "mlp_train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "mlp_train_mcc = matthews_corrcoef(y_train, y_train_pred)\n",
    "mlp_train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "# Test set performance\n",
    "mlp_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "mlp_test_mcc = matthews_corrcoef(y_test, y_test_pred)\n",
    "mlp_test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Accuracy: %s' % mlp_train_accuracy)\n",
    "print('- MCC: %s' % mlp_train_mcc)\n",
    "print('- F1 score: %s' % mlp_train_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Accuracy: %s' % mlp_test_accuracy)\n",
    "print('- MCC: %s' % mlp_test_mcc)\n",
    "print('- F1 score: %s' % mlp_test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa889b9",
   "metadata": {},
   "source": [
    "# Support vector machine (Radial basis function kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e252c73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Accuracy: 0.9986366734832992\n",
      "- MCC: 0.9972759961766186\n",
      "- F1 score: 0.9986366367265247\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9243353783231084\n",
      "- MCC: 0.8520646375719324\n",
      "- F1 score: 0.9243752542765614\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create and train the SVM classifier with RBF kernel\n",
    "svm_rbf = SVC(gamma=2, C=1)\n",
    "svm_rbf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = svm_rbf.predict(X_train_tfidf)\n",
    "y_test_pred = svm_rbf.predict(tfidf_vect.transform(X_test))\n",
    "\n",
    "# Training set performance\n",
    "svm_rbf_train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "svm_rbf_train_mcc = matthews_corrcoef(y_train, y_train_pred)\n",
    "svm_rbf_train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "# Test set performance\n",
    "svm_rbf_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "svm_rbf_test_mcc = matthews_corrcoef(y_test, y_test_pred)\n",
    "svm_rbf_test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Accuracy: %s' % svm_rbf_train_accuracy)\n",
    "print('- MCC: %s' % svm_rbf_train_mcc)\n",
    "print('- F1 score: %s' % svm_rbf_train_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Accuracy: %s' % svm_rbf_test_accuracy)\n",
    "print('- MCC: %s' % svm_rbf_test_mcc)\n",
    "print('- F1 score: %s' % svm_rbf_test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde800eb",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9dc1571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Accuracy: 0.9706884798909339\n",
      "- MCC: 0.9414860284894111\n",
      "- F1 score: 0.9706813922050835\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9100204498977505\n",
      "- MCC: 0.8210115644271592\n",
      "- F1 score: 0.9097130493697667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score\n",
    "\n",
    "# Create and train the Naive Bayes classifier\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = naive_bayes.predict(X_train_tfidf)\n",
    "y_test_pred = naive_bayes.predict(tfidf_vect.transform(X_test))\n",
    "\n",
    "# Training set performance\n",
    "naive_bayes_train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "naive_bayes_train_mcc = matthews_corrcoef(y_train, y_train_pred)\n",
    "naive_bayes_train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "# Test set performance\n",
    "naive_bayes_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "naive_bayes_test_mcc = matthews_corrcoef(y_test, y_test_pred)\n",
    "naive_bayes_test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Accuracy: %s' % naive_bayes_train_accuracy)\n",
    "print('- MCC: %s' % naive_bayes_train_mcc)\n",
    "print('- F1 score: %s' % naive_bayes_train_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Accuracy: %s' % naive_bayes_test_accuracy)\n",
    "print('- MCC: %s' % naive_bayes_test_mcc)\n",
    "print('- F1 score: %s' % naive_bayes_test_f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a027ee5",
   "metadata": {},
   "source": [
    "# Build Stacked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3de5aa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Accuracy: 0.9993183367416496\n",
      "- MCC: 0.998637106019357\n",
      "- F1 score: 0.9993183278695598\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9468302658486708\n",
      "- MCC: 0.893335346823719\n",
      "- F1 score: 0.9468302658486708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define a list of estimators\n",
    "estimator_list = [\n",
    "    ('svm_rbf', svm_rbf),\n",
    "    ('dt', dt),\n",
    "    ('rf', rf),\n",
    "    ('mlp', mlp),\n",
    "    ('naive_bayes', naive_bayes)\n",
    "]\n",
    "\n",
    "# Build the stacked model with a final Logistic Regression estimator\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=estimator_list, final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "# Train the stacked model\n",
    "stack_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = stack_model.predict(X_train_tfidf)\n",
    "y_test_pred = stack_model.predict(tfidf_vect.transform(X_test))\n",
    "\n",
    "# Training set model performance\n",
    "stack_model_train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "stack_model_train_mcc = matthews_corrcoef(y_train, y_train_pred)\n",
    "stack_model_train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "# Test set model performance\n",
    "stack_model_test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "stack_model_test_mcc = matthews_corrcoef(y_test, y_test_pred)\n",
    "stack_model_test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Accuracy: %s' % stack_model_train_accuracy)\n",
    "print('- MCC: %s' % stack_model_train_mcc)\n",
    "print('- F1 score: %s' % stack_model_train_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Accuracy: %s' % stack_model_test_accuracy)\n",
    "print('- MCC: %s' % stack_model_test_mcc)\n",
    "print('- F1 score: %s' % stack_model_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a832abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Naive Bayes metrics to the dictionaries\n",
    "acc_train_list = {\n",
    "    'svm_rbf': svm_rbf_train_accuracy,\n",
    "    'dt': dt_train_accuracy,\n",
    "    'rf': rf_train_accuracy,\n",
    "    'mlp': mlp_train_accuracy,\n",
    "    'naive_bayes': naive_bayes_train_accuracy,  # Add Naive Bayes accuracy\n",
    "    'stack': stack_model_train_accuracy\n",
    "}\n",
    "\n",
    "mcc_train_list = {\n",
    "    'svm_rbf': svm_rbf_train_mcc,\n",
    "    'dt': dt_train_mcc,\n",
    "    'rf': rf_train_mcc,\n",
    "    'mlp': mlp_train_mcc,\n",
    "    'naive_bayes': naive_bayes_train_mcc,  # Add Naive Bayes MCC\n",
    "    'stack': stack_model_train_mcc\n",
    "}\n",
    "\n",
    "f1_train_list = {\n",
    "    'svm_rbf': svm_rbf_train_f1,\n",
    "    'dt': dt_train_f1,\n",
    "    'rf': rf_train_f1,\n",
    "    'mlp': mlp_train_f1,\n",
    "    'naive_bayes': naive_bayes_train_f1,  # Add Naive Bayes F1 score\n",
    "    'stack': stack_model_train_f1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4a1c711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svm_rbf': 0.9972759961766186,\n",
       " 'dt': 0.837202504494492,\n",
       " 'rf': 0.9878030529094353,\n",
       " 'mlp': 0.9836699249289926,\n",
       " 'naive_bayes': 0.9414860284894111,\n",
       " 'stack': 0.998637106019357}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcc_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92496d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Accuracy       MCC        F1\n",
      "svm_rbf      0.998637  0.997276  0.998637\n",
      "dt           0.912065  0.837203  0.911544\n",
      "rf           0.993865  0.987803  0.993865\n",
      "mlp          0.991820  0.983670  0.991821\n",
      "naive_bayes  0.970688  0.941486  0.970681\n",
      "stack        0.999318  0.998637  0.999318\n"
     ]
    }
   ],
   "source": [
    "acc_df = pd.DataFrame.from_dict(acc_train_list, orient='index', columns=['Accuracy'])\n",
    "mcc_df = pd.DataFrame.from_dict(mcc_train_list, orient='index', columns=['MCC'])\n",
    "f1_df = pd.DataFrame.from_dict(f1_train_list, orient='index', columns=['F1'])\n",
    "\n",
    "# Concatenate the DataFrames into a single DataFrame\n",
    "df = pd.concat([acc_df, mcc_df, f1_df], axis=1)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ff13ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking model saved to stacked_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the stacking model to a pickle file\n",
    "joblib.dump(stack_model, 'stacked_model.pkl')\n",
    "\n",
    "# Verify that the model has been saved\n",
    "print(\"Stacking model saved to stacked_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2ca7ece",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Increase the size of the plots\u001b[39;00m\n\u001b[0;32m      4\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m15\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Increase the size of the plots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 15))\n",
    "\n",
    "# Plot Accuracy\n",
    "axes[0].bar(df.index, df['Accuracy'], color='skyblue')\n",
    "axes[0].set_xlabel('Classifier')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy Comparison')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot MCC\n",
    "axes[1].bar(df.index, df['MCC'], color='lightgreen')\n",
    "axes[1].set_xlabel('Classifier')\n",
    "axes[1].set_ylabel('MCC')\n",
    "axes[1].set_title('MCC Comparison')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot F1-score\n",
    "axes[2].bar(df.index, df['F1'], color='lightcoral')\n",
    "axes[2].set_xlabel('Classifier')\n",
    "axes[2].set_ylabel('F1-score')\n",
    "axes[2].set_title('F1-score Comparison')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust spacing between plots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e057bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the pickle file\n",
    "loaded_model = joblib.load('stacked_model.pkl')\n",
    "\n",
    "# The comment you want to make a prediction for\n",
    "comment = [\"subscribe here\"]\n",
    "\n",
    "# Transform the comment into a TF-IDF vector\n",
    "comment_tfidf = tfidf_vect.transform(comment)\n",
    "\n",
    "# Use the loaded model to make predictions\n",
    "predictions = loaded_model.predict(comment_tfidf)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d9897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_vect saved to tfidf_vect.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the stacking model to a pickle file\n",
    "joblib.dump(tfidf_vect, 'tfidf_vect.pkl')\n",
    "\n",
    "# Verify that the model has been saved\n",
    "print(\"tfidf_vect saved to tfidf_vect.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944bda9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
